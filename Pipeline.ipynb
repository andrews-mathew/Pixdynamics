{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from paddleocr import PaddleOCR\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- Configuration ---\n",
    "@dataclass\n",
    "class Config:\n",
    "    IMAGE_FOLDER = r\"D:\\New folder (2)\"   # Input folder\n",
    "    OUTPUT_FOLDER = r\"D:\\ocr_new_files\"         # Output folder for OCR text\n",
    "    SUPPORTED_EXTENSIONS = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --- Logging setup ---\n",
    "os.makedirs(config.OUTPUT_FOLDER, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(config.OUTPUT_FOLDER, 'ocr_run.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Initialize PaddleOCR ---\n",
    "print(\"Initializing PaddleOCR reader...\")\n",
    "try:\n",
    "    ocr_reader = PaddleOCR(lang='en')\n",
    "    print(\"âœ… PaddleOCR reader initialized successfully.\")\n",
    "    logger.info(\"PaddleOCR initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error initializing PaddleOCR: {e}\")\n",
    "    logger.error(f\"Error initializing PaddleOCR: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- OCR extraction function ---\n",
    "class SimpleLineOCRExtractor:\n",
    "    \"\"\"Performs line-structured OCR extraction using PaddleOCR\"\"\"\n",
    "    @staticmethod\n",
    "    def extract_with_line_structure(image_path: str):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = ocr_reader.predict(image_path)\n",
    "            extracted_texts = []\n",
    "            if result and result[0] and 'rec_texts' in result[0]:\n",
    "                extracted_texts = result[0]['rec_texts']\n",
    "            structured_text = \"\\n\".join([text.strip() for text in extracted_texts if text.strip()])\n",
    "            latency = time.time() - start_time\n",
    "            return structured_text, latency\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OCR failed for {image_path}: {e}\")\n",
    "            return \"\", time.time() - start_time\n",
    "\n",
    "# --- Save OCR text (without extraction details) ---\n",
    "def perform_ocr_and_save(image_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    extracted_text, processing_time = SimpleLineOCRExtractor.extract_with_line_structure(image_path)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_text_file = os.path.join(output_dir, f\"{base_name}_ocr.txt\")\n",
    "\n",
    "    # âœ… Save ONLY the extracted text (no details at the bottom)\n",
    "    with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_text)\n",
    "\n",
    "    return extracted_text, processing_time\n",
    "\n",
    "# --- Main driver function ---\n",
    "def main():\n",
    "    if not os.path.isdir(config.IMAGE_FOLDER):\n",
    "        print(f\"âŒ Image folder not found: {config.IMAGE_FOLDER}\")\n",
    "        return\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(config.IMAGE_FOLDER)\n",
    "        if f.lower().endswith(config.SUPPORTED_EXTENSIONS)\n",
    "    ]\n",
    "    if not image_files:\n",
    "        print(\"âš ï¸ No image files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ–¼ï¸ Found {len(image_files)} images to process.\\n\")\n",
    "\n",
    "    for image_name in image_files:\n",
    "        image_path = os.path.join(config.IMAGE_FOLDER, image_name)\n",
    "        print(f\"--- Processing: {image_name} ---\")\n",
    "        extracted_text, latency = perform_ocr_and_save(image_path, config.OUTPUT_FOLDER)\n",
    "        lines = extracted_text.split(\"\\n\") if extracted_text else []\n",
    "        print(f\"  â†’ OCR completed in {latency:.2f}s | {len(lines)} lines extracted\")\n",
    "        if lines:\n",
    "            print(\"  Preview (first 5 lines):\")\n",
    "            for i, line in enumerate(lines[:5]):\n",
    "                print(f\"   {i+1:2d}: {line}\")\n",
    "            if len(lines) > 5:\n",
    "                print(f\"   ... and {len(lines) - 5} more lines\\n\")\n",
    "\n",
    "    print(\"\\nâœ… OCR extraction complete!\")\n",
    "    print(f\"ðŸ—‚ï¸ Clean text files saved in: {config.OUTPUT_FOLDER}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdad108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Ollama python library and pandas\n",
    "!pip install ollama pandas\n",
    "\n",
    "# 2. Install Ollama System Binary\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start ollama serve as a background process\n",
    "process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"â³ Starting Ollama server...\")\n",
    "time.sleep(10) # Give it a moment to start\n",
    "\n",
    "print(\"â¬‡ï¸ Pulling Model \")\n",
    "!ollama pull gemma3:4b\n",
    "print(\"âœ… Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import ollama\n",
    "from typing import Dict, Optional, Any, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    INPUT_TEXT_FOLDER = \"ocr_texts\"\n",
    "    OUTPUT_FOLDER = \"slm_json_results\"\n",
    "    MODEL_NAME = \"gemma3:4b\"\n",
    "    LLM_TIMEOUT = 60\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --- VERSION 5: PROMPT WITH DATE SORTER SUPPORT ---\n",
    "# --- VERSION 6: OPTIMIZED CHAIN-OF-THOUGHT PROMPT ---\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert Document Intelligence Agent specialized in Indian Driving Licenses. Your task is to extract structured data from raw OCR text with 100% precision.\n",
    "\n",
    "### 1. EXTRACTION RULES (STRICT):\n",
    "\n",
    "**A. NAME (CRITICAL):**\n",
    "* **Goal:** Extract the License Holder's Name.\n",
    "* **Logic:** The Holder's name is usually the **first** personal name appearing on the card, often below \"DL No\" or \"Name\".\n",
    "* **NEGATIVE CONSTRAINT:** IGNORE any line starting with or containing: \"S/o\", \"D/o\", \"W/o\", \"C/o\", \"Father\", \"Husband\", \"Son of\", \"Wife of\".\n",
    "* *Example:* If you see \"Name: T VIJAYA\" and \"S/W/D: R SREEDHAR\", extract \"T VIJAYA\".\n",
    "\n",
    "**B. DATES (Contextual):**\n",
    "* **Date of Birth (DOB):** Look for 'DOB', 'Birth', or the **oldest** date (e.g., 1970-2005).\n",
    "* **Issue Date:** Look for 'DOI', 'Iss', 'Date of Issue'. Usually in the past.\n",
    "* **Validity:** Look for 'Val', 'Exp', 'Valid Till'. Usually in the future (e.g., 2026-2045).\n",
    "* *Format:* Standardize to DD-MM-YYYY.\n",
    "\n",
    "**C. BLOOD GROUP (Anti-Hallucination):**\n",
    "* **Rule:** ONLY extract if you clearly see a blood group pattern (A+, B+, O+, AB+, etc.).\n",
    "* **Hallucination Check:** If the field is blank, missing, or contains \"Unknown\"/\"None\", return `null`.\n",
    "* **Correction:** Treat '0+' (zero) as 'O+'. Treat 'B4' as 'B+'.\n",
    "\n",
    "**D. ADDRESS:**\n",
    "* Extract the full continuous address block. Ignore lines that are clearly headers (like \"Union of India\").\n",
    "\n",
    "### 2. JSON OUTPUT FORMAT:\n",
    "Return ONLY a valid JSON object. No markdown, no explanations.\n",
    "{\n",
    "    \"name\": \"string or null\",\n",
    "    \"blood_group\": \"string or null\",\n",
    "    \"address\": \"string or null\",\n",
    "    \"date_of_birth\": \"DD-MM-YYYY or null\",\n",
    "    \"issue_date\": \"DD-MM-YYYY or null\",\n",
    "    \"validity\": \"DD-MM-YYYY or null\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- LLM Analyzer Class ---\n",
    "class LLMAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.client = ollama.Client()\n",
    "\n",
    "    def clean_blood_group(self, bg: str) -> Optional[str]:\n",
    "        \"\"\"Fixes common OCR typos in Blood Group.\"\"\"\n",
    "        if not bg or str(bg).lower() == \"null\": return None\n",
    "        bg = bg.upper().strip()\n",
    "        bg = bg.replace('0', 'O').replace('Q', 'O').replace('4', '+').replace('VE', '')\n",
    "        if re.match(r'^(A|B|AB|O)[+-]$', bg):\n",
    "            return bg\n",
    "        return None\n",
    "\n",
    "    def parse_date(self, date_str: str) -> Optional[datetime]:\n",
    "        \"\"\"Helper to parse DD-MM-YYYY strings.\"\"\"\n",
    "        if not date_str: return None\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%d-%m-%Y\")\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def sort_and_fix_dates(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        DATE SORTER LOGIC:\n",
    "        1. Collects DOB, Issue Date, and Validity.\n",
    "        2. Sorts them chronologically.\n",
    "        3. Re-assigns based on logic: DOB (Oldest) < Issue (Middle) < Validity (Newest/Future).\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Collect all valid dates found by the LLM\n",
    "        found_dates = []\n",
    "        for key in [\"date_of_birth\", \"issue_date\", \"validity\"]:\n",
    "            d_str = data.get(key)\n",
    "            d_obj = self.parse_date(d_str)\n",
    "            if d_obj:\n",
    "                found_dates.append(d_obj)\n",
    "\n",
    "        # Remove duplicates and sort\n",
    "        found_dates = sorted(list(set(found_dates)))\n",
    "\n",
    "        count = len(found_dates)\n",
    "\n",
    "        # 2. Apply Sorting Logic\n",
    "        if count == 3:\n",
    "            # Perfect Case: 3 dates found\n",
    "            data[\"date_of_birth\"] = found_dates[0].strftime(\"%d-%m-%Y\") # Oldest\n",
    "            data[\"issue_date\"] = found_dates[1].strftime(\"%d-%m-%Y\")    # Middle\n",
    "            data[\"validity\"] = found_dates[2].strftime(\"%d-%m-%Y\")      # Newest\n",
    "\n",
    "        elif count == 2:\n",
    "            # Ambiguous Case: We have 2 dates. Usually DOB+Issue OR Issue+Validity.\n",
    "            # Heuristic: Check if the newest date is far in the future (Validity)\n",
    "            # or if the oldest date is very old (DOB).\n",
    "\n",
    "            oldest = found_dates[0]\n",
    "            newest = found_dates[1]\n",
    "            today = datetime.now()\n",
    "\n",
    "            # If newest is in the future, it is definitely Validity\n",
    "            if newest > today:\n",
    "                data[\"validity\"] = newest.strftime(\"%d-%m-%Y\")\n",
    "                # The other date is likely Issue Date (unless it's very old, e.g. < 1980)\n",
    "                if oldest.year < (today.year - 18):\n",
    "                    # If > 18 years ago, safest to assume DOB (or an old Issue date).\n",
    "                    # Let's verify against the LLM's original guess.\n",
    "                    # For safety in this pipeline, we often assume Issue Date if it's recent enough.\n",
    "                    data[\"issue_date\"] = oldest.strftime(\"%d-%m-%Y\")\n",
    "                    data[\"date_of_birth\"] = None # Reset potential error\n",
    "                else:\n",
    "                     data[\"issue_date\"] = oldest.strftime(\"%d-%m-%Y\")\n",
    "            else:\n",
    "                # Both dates are in the past. Likely DOB and Issue Date.\n",
    "                data[\"date_of_birth\"] = oldest.strftime(\"%d-%m-%Y\")\n",
    "                data[\"issue_date\"] = newest.strftime(\"%d-%m-%Y\")\n",
    "                data[\"validity\"] = None\n",
    "\n",
    "        return data\n",
    "\n",
    "    def analyze_with_llm(self, text: str) -> Tuple[Dict, float]:\n",
    "        if not text.strip(): return {\"error\": \"Empty text file\"}, 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"OCR Text:\\n{text}\"}\n",
    "            ]\n",
    "\n",
    "            response = self.client.chat(\n",
    "                model=config.MODEL_NAME,\n",
    "                messages=messages,\n",
    "                options={'temperature': 0.01, 'num_ctx': 2048, 'timeout': config.LLM_TIMEOUT * 1000}\n",
    "            )\n",
    "\n",
    "            content = response['message']['content']\n",
    "            json_result = self.extract_json_from_response(content)\n",
    "            latency = time.time() - start_time\n",
    "\n",
    "            return json_result, latency\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            return {\"error\": str(e)}, time.time() - start_time\n",
    "\n",
    "    def extract_json_from_response(self, content: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            content = re.sub(r'```json\\s*', '', content, flags=re.IGNORECASE)\n",
    "            content = re.sub(r'```', '', content)\n",
    "            start, end = content.find('{'), content.rfind('}') + 1\n",
    "            if start != -1 and end != -1:\n",
    "                content = content[start:end]\n",
    "\n",
    "            result = json.loads(content)\n",
    "\n",
    "            # --- Apply Cleaners & Sorters ---\n",
    "            result[\"blood_group\"] = self.clean_blood_group(result.get(\"blood_group\"))\n",
    "            result = self.sort_and_fix_dates(result) # <--- NEW DATE SORTER CALLED HERE\n",
    "\n",
    "            # Sanitize nulls\n",
    "            for k, v in result.items():\n",
    "                if isinstance(v, str) and v.lower() == \"null\": result[k] = None\n",
    "\n",
    "            # Filter final fields (we exclude DOB from final output if you don't want it,\n",
    "            # but keeping it is useful for debugging. Here I stick to your original 5 fields).\n",
    "            required_fields = [\"name\", \"blood_group\", \"address\", \"issue_date\", \"validity\"]\n",
    "            return {k: result.get(k) for k in required_fields}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Extraction error: {str(e)}\"}\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    if not os.path.exists(config.INPUT_TEXT_FOLDER):\n",
    "        os.makedirs(config.INPUT_TEXT_FOLDER)\n",
    "\n",
    "    os.makedirs(config.OUTPUT_FOLDER, exist_ok=True)\n",
    "    text_files = [f for f in os.listdir(config.INPUT_TEXT_FOLDER) if f.endswith('.txt')]\n",
    "\n",
    "    print(f\"ðŸ”Ž Found {len(text_files)} files. Running Version 5 (With Date Sorter)...\")\n",
    "    analyzer = LLMAnalyzer()\n",
    "    all_results = []\n",
    "\n",
    "    for filename in text_files:\n",
    "        filepath = os.path.join(config.INPUT_TEXT_FOLDER, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "        print(f\"\\nðŸ“„ Processing: {filename}\")\n",
    "        extracted_data, duration = analyzer.analyze_with_llm(raw_text)\n",
    "\n",
    "        if \"error\" not in extracted_data:\n",
    "            print(f\"   âœ… Name: {extracted_data.get('name')}\")\n",
    "            print(f\"   âœ… Issue: {extracted_data.get('issue_date')} | Valid: {extracted_data.get('validity')}\")\n",
    "        else:\n",
    "            print(f\"   âŒ Failed: {extracted_data['error']}\")\n",
    "\n",
    "        record = {\"source_file\": filename, \"processing_time_seconds\": round(duration, 2), **extracted_data}\n",
    "        all_results.append(record)\n",
    "\n",
    "    json_path = os.path.join(config.OUTPUT_FOLDER, \"gemma3_4b_v5_sorted.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Done! Results saved to {json_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
